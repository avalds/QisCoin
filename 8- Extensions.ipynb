{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Possible extensions of QisCoin\n",
    "\n",
    "In this notebook, we suggest possible extensions of the **QisCoin** game and further experiments that could be run using reinforcement learning agents."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Extension to the game QisCoin\n",
    "\n",
    "### More didactic feedback\n",
    "To improve the educational value of **QisCoin**, it would be possible to add a richer feedback to the player. At the moment, the result of the game is simply a score. It could be possible to offer to a learning student a more precise explanation of the output of measurement, distinguish for instance if a mistake was just the result of a random outcome or a miscomputed deterministic output, and providing a guideline on how the correct answer could have been obtained.\n",
    "\n",
    "### More difficult circuits\n",
    "Right now **QisCoin** only applies gates to a single qubit, however the game can be enhanced by adding more qubits and entangligh the qubit of interest (the coin). It could also be possible to allow the user to perform some measurements on the other qubits in the circuits in order to obtain useful information about the final measurement.\n",
    "\n",
    "### More engaging highscore\n",
    "Adding timing to the higscore could make the game more engaging, especially on the *easy* game mode where all the gates lead to a deterministic measurement of the qubit. This would allow to rank the best players with how much time they used to read the circuits."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Extension on the interaction between QisCoin and AI agents\n",
    "\n",
    "### Better hyper-parameter tuning on the agents\n",
    "A first simple improvement would be performing a better choice about which reinforcement learning model to use for solving the problem, and then properly tweaking the hyper-parameters of the chosen agent. This may entail a quite computationally-expensive exploration of the hyper-parameter space that would allow us to understand the actual possibility of a reinforcement learning agent on this scenario.\n",
    "\n",
    "### Dealing with uncertainty\n",
    "A big challenge for the learning agent is the intrinsic uncertainity in our scenario, which is not due to random noise and fluctuations that we want to filter out, but which is a feature of the scenario itself. It may be possible to consider standard uncertainty estimation techniques, such as *Bayesian modelling*, to try to manage this form of uncertainty.\n",
    "\n",
    "### Integrating quantum logic\n",
    "An even more interesting option may be to allow the agent to rely on quantum logic itself to model observed quantum phenomena. Instead of relying on standard techniques to model uncertainty, we may have the agent devising internal quantum model (such as, *qiskit circuits*) to represent the phenomena it is observing.\n",
    "\n",
    "### Extending the complexity of the environment\n",
    "**QisCoin** represents a very simple quantum reinforcement learning problem. It boils down to a *contextual 2-arms bandit*, and it constitutes a basic one-step reinforcement learning scenario. More intriguing problems would involve multi-steps simulations with sparse rewards and dynamics partially explained by quantum mechanics."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
